#!/usr/bin/env python3
"""
ðŸš€ BERZERK Real-Time RSS Monitor - Surveillance RSS Quasi-InstantanÃ©e
=====================================================================

Ce systÃ¨me avancÃ© surveille les flux RSS en temps quasi-rÃ©el avec :
- Polling haute frÃ©quence (30-60 secondes)
- Optimisations HTTP (ETags, Last-Modified, conditional requests)
- Threading asynchrone pour Ã©viter les blocages
- DÃ©tection intelligente des changements
- Analyse automatique instantanÃ©e

Architecture Temps RÃ©el :
- Pas d'attente bloquante (utilise threading)
- RÃ©activitÃ© Ã©vÃ©nementielle
- Gestion des erreurs robuste
- IntÃ©gration avec le pipeline BERZERK existant

Usage: python real_time_rss_monitor.py [poll_interval_seconds]
ArrÃªt: Ctrl+C
"""

import asyncio
import threading
import time
import requests
import feedparser
import hashlib
import sqlite3
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set, Any
from dataclasses import dataclass
from email.utils import parsedate_to_datetime
import sys
import traceback

# Import des modules BERZERK
from berzerk_lab import RSS_FEEDS, init_db, get_article_text, analyze_news_with_llm
from orchestrator import run_berzerk_pipeline


@dataclass
class FeedState:
    """Ã‰tat d'un flux RSS avec optimisations HTTP"""
    url: str
    last_modified: Optional[str] = None
    etag: Optional[str] = None
    last_check: Optional[datetime] = None
    last_content_hash: Optional[str] = None
    consecutive_errors: int = 0
    # articles_cache supprimÃ© - causait des doublons aprÃ¨s redÃ©marrage
    
    def should_check(self, min_interval: int) -> bool:
        """DÃ©termine si le flux doit Ãªtre vÃ©rifiÃ© maintenant"""
        if self.last_check is None:
            return True
        
        # Algorithme adaptatif : plus d'erreurs = moins de vÃ©rifications
        if self.consecutive_errors > 0:
            penalty = min(self.consecutive_errors * 30, 300)  # Max 5 minutes de penalty
            return (datetime.now() - self.last_check).total_seconds() > (min_interval + penalty)
        
        return (datetime.now() - self.last_check).total_seconds() >= min_interval


class RealTimeRSSMonitor:
    """Surveillant RSS temps rÃ©el avec optimisations avancÃ©es"""
    
    def __init__(self, poll_interval: int = 30, capital: float = 25000.0):
        self.poll_interval = poll_interval  # En secondes
        self.capital = capital
        self.feeds_state: Dict[str, FeedState] = {}
        self.running = False
        self.processed_articles: Set[str] = set()
        self.stats = {
            'total_checks': 0,
            'new_articles_found': 0,
            'analyses_completed': 0,
            'errors': 0,
            'start_time': datetime.now()
        }
        
        # Configuration des requÃªtes HTTP optimisÃ©es
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'BERZERK-RSS-Monitor/1.0 (Real-Time News Analysis)',
            'Accept': 'application/rss+xml, application/xml, text/xml',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
        
        # Initialisation
        self.init_feeds_state()
        self.load_processed_articles()
        
        print(f"ðŸš€ BERZERK Real-Time RSS Monitor initialisÃ©")
        print(f"   âš¡ Polling: {poll_interval} secondes (haute frÃ©quence)")
        print(f"   ðŸ“ˆ Flux RSS: {len(RSS_FEEDS)} source (Bloomberg uniquement)")
        for feed_name, feed_url in RSS_FEEDS.items():
            print(f"   ðŸ“¡ {feed_name}: {feed_url}")
        print(f"   ðŸ’° Capital: {capital:,.2f}â‚¬")
        print(f"   ðŸ”„ Optimisations: ETags, Last-Modified, Threading")
        print(f"   ðŸ“Š Articles dÃ©jÃ  traitÃ©s: {len(self.processed_articles)}")
        print("-" * 70)
    
    def init_feeds_state(self):
        """Initialise l'Ã©tat de chaque flux RSS"""
        for feed_name, feed_url in RSS_FEEDS.items():
            self.feeds_state[feed_url] = FeedState(url=feed_url)
    
    def load_processed_articles(self):
        """Charge les articles dÃ©jÃ  traitÃ©s depuis la base de donnÃ©es"""
        try:
            conn = sqlite3.connect('berzerk.db')
            cursor = conn.cursor()
            # Charger TOUS les articles existants (peu importe le statut)
            cursor.execute("SELECT link FROM articles")
            self.processed_articles = {row[0] for row in cursor.fetchall()}
            conn.close()
            
            self.log(f"ðŸ“¥ {len(self.processed_articles)} articles existants chargÃ©s")
            self.log("ðŸŽ¯ Seuls les vrais nouveaux articles RSS seront traitÃ©s")
        except Exception as e:
            self.log(f"âš ï¸ Erreur lors du chargement des articles traitÃ©s: {e}")
            self.processed_articles = set()
    
    def log(self, message: str, level: str = "INFO"):
        """Logger avec timestamp et niveau"""
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        level_emoji = {
            "INFO": "â„¹ï¸",
            "SUCCESS": "âœ…",
            "WARNING": "âš ï¸",
            "ERROR": "âŒ",
            "DETECTION": "ðŸ”"
        }.get(level, "ðŸ“")
        print(f"âš¡ [{timestamp}] {level_emoji} {message}")
    
    def calculate_content_hash(self, content: str) -> str:
        """Calcule un hash du contenu pour dÃ©tecter les changements"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def check_feed_optimized(self, feed_state: FeedState) -> List[Dict]:
        """VÃ©rifie un flux RSS avec optimisations HTTP"""
        try:
            headers = {}
            
            # Optimisations HTTP conditionnelles
            if feed_state.etag:
                headers['If-None-Match'] = feed_state.etag
            if feed_state.last_modified:
                headers['If-Modified-Since'] = feed_state.last_modified
            
            # RequÃªte HTTP avec timeout court
            response = self.session.get(
                feed_state.url, 
                headers=headers, 
                timeout=10
            )
            
            feed_state.last_check = datetime.now()
            self.stats['total_checks'] += 1
            
            # Gestion des codes de statut HTTP
            if response.status_code == 304:
                # Not Modified - aucun changement
                self.log(f"ðŸ”„ Pas de changement: {feed_state.url[:50]}...")
                feed_state.consecutive_errors = 0
                return []
            
            if response.status_code != 200:
                raise Exception(f"HTTP {response.status_code}")
            
            # Mise Ã  jour des mÃ©tadonnÃ©es de cache
            feed_state.etag = response.headers.get('ETag')
            feed_state.last_modified = response.headers.get('Last-Modified')
            
            # VÃ©rification du hash du contenu
            content_hash = self.calculate_content_hash(response.text)
            if feed_state.last_content_hash == content_hash:
                self.log(f"ðŸ”„ Contenu identique: {feed_state.url[:50]}...")
                feed_state.consecutive_errors = 0
                return []
            
            feed_state.last_content_hash = content_hash
            
            # Parsing RSS
            feed = feedparser.parse(response.text)
            
            if feed.bozo:
                self.log(f"âš ï¸ Feed mal formÃ©: {feed_state.url[:50]}...")
            
            # Filtrage des nouveaux articles - LOGIQUE SIMPLIFIÃ‰E
            new_articles = []
            for entry in feed.entries:
                article_link = entry.get('link', '')
                
                # âœ… CORRECTION : Ne vÃ©rifier que processed_articles (pas articles_cache)
                if article_link and article_link not in self.processed_articles:
                    
                    # Marquer immÃ©diatement comme traitÃ© pour Ã©viter les doublons
                    self.processed_articles.add(article_link)
                    
                    # Extraction des mÃ©tadonnÃ©es
                    published_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'published'):
                        try:
                            published_date = parsedate_to_datetime(entry.published)
                        except:
                            published_date = datetime.now()
                    else:
                        published_date = datetime.now()
                    
                    article = {
                        'title': entry.get('title', 'Titre non disponible'),
                        'link': article_link,
                        'published_date': published_date,
                        'summary': entry.get('summary', ''),
                        'source': feed_state.url,
                        'discovered_at': datetime.now()
                    }
                    
                    new_articles.append(article)
            
            # SuccÃ¨s - reset des erreurs
            feed_state.consecutive_errors = 0
            
            if new_articles:
                self.log(f"ðŸŽ¯ {len(new_articles)} VRAIS nouveaux articles dÃ©tectÃ©s sur {feed_state.url[:50]}...", "DETECTION")
                self.stats['new_articles_found'] += len(new_articles)
            
            return new_articles
            
        except Exception as e:
            feed_state.consecutive_errors += 1
            self.stats['errors'] += 1
            self.log(f"âŒ Erreur sur {feed_state.url[:50]}: {e}")
            return []
    
    def store_article(self, article: Dict):
        """Stocke un article dans la base de donnÃ©es"""
        try:
            conn = sqlite3.connect('berzerk.db')
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR IGNORE INTO articles 
                (title, link, published_date, source, status, published_str)
                VALUES (?, ?, ?, ?, 'pending', ?)
            """, (
                article['title'],
                article['link'],
                article['published_date'].isoformat(),
                article['source'],
                article.get('summary', 'RSS Feed')  # Utilise summary comme published_str
            ))
            
            conn.commit()
            conn.close()
            
            if cursor.rowcount > 0:
                self.log(f"ðŸ’¾ Article stockÃ©: {article['title'][:50]}...")
            
        except Exception as e:
            self.log(f"âŒ Erreur stockage: {e}")
    
    def analyze_article_realtime(self, article: Dict) -> Optional[Dict]:
        """Analyse un article en temps rÃ©el avec le pipeline BERZERK"""
        try:
            self.log(f"ðŸ¤– Analyse TEMPS RÃ‰EL: {article['title'][:50]}...")
            
            # Lancement du pipeline complet
            result = run_berzerk_pipeline(article['link'], self.capital)
            
            if result and 'final_decision' in result:
                # Sauvegarde de la dÃ©cision
                self.save_decision_to_db(article['link'], result['final_decision'])
                
                # Marquer comme traitÃ©
                self.processed_articles.add(article['link'])
                
                # Statistiques
                self.stats['analyses_completed'] += 1
                
                # Affichage du rÃ©sultat
                decision = result['final_decision']
                action = decision.get('decision', 'INCONNU')
                
                if action == 'ACHETER':
                    self.log(f"ðŸš€ ACHAT IDENTIFIÃ‰: {article['title'][:50]}...", "SUCCESS")
                    ticker = decision.get('ticker_cible', 'N/A')
                    allocation = decision.get('allocation_pourcentage', 0)
                    self.log(f"   ðŸ’° Ticker: {ticker}, Allocation: {allocation}%")
                elif action == 'SURVEILLER':
                    self.log(f"ðŸ‘€ SURVEILLANCE: {article['title'][:50]}...")
                else:
                    self.log(f"âœ… Analyse terminÃ©e: {action}")
                
                return result
            else:
                self.log(f"âš ï¸ Ã‰chec d'analyse: {article['title'][:50]}...")
                return None
                
        except Exception as e:
            self.log(f"âŒ Erreur analyse temps rÃ©el: {e}")
            return None
    
    def save_decision_to_db(self, article_link: str, decision: Dict):
        """Sauvegarde la dÃ©cision dans la base de donnÃ©es"""
        try:
            conn = sqlite3.connect('berzerk.db')
            cursor = conn.cursor()
            
            cursor.execute("""
                UPDATE articles 
                SET decision_json = ?, 
                    status = 'analyzed', 
                    analyzed_at = ?
                WHERE link = ?
            """, (
                json.dumps(decision, ensure_ascii=False),
                datetime.now().isoformat(),
                article_link
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.log(f"âŒ Erreur sauvegarde dÃ©cision: {e}")
    
    def monitoring_thread(self):
        """Thread principal de surveillance"""
        self.log("ðŸ”„ Thread de surveillance dÃ©marrÃ©")
        
        while self.running:
            try:
                # VÃ©rifier chaque flux RSS
                for feed_url, feed_state in self.feeds_state.items():
                    if not self.running:
                        break
                    
                    # VÃ©rifier si le flux doit Ãªtre contrÃ´lÃ©
                    if feed_state.should_check(self.poll_interval):
                        new_articles = self.check_feed_optimized(feed_state)
                        
                        # Traiter les nouveaux articles
                        for article in new_articles:
                            if not self.running:
                                break
                            
                            # Stockage en base
                            self.store_article(article)
                            
                            # Analyse immÃ©diate en arriÃ¨re-plan
                            threading.Thread(
                                target=self.analyze_article_realtime,
                                args=(article,),
                                daemon=True
                            ).start()
                
                # Pause courte avant le prochain cycle
                time.sleep(1)
                
            except Exception as e:
                self.log(f"âŒ Erreur dans le thread de surveillance: {e}")
                time.sleep(5)  # Pause plus longue en cas d'erreur
        
        self.log("ðŸ›‘ Thread de surveillance arrÃªtÃ©")
    
    def display_stats(self):
        """Affiche les statistiques en temps rÃ©el"""
        uptime = datetime.now() - self.stats['start_time']
        
        print("\n" + "="*50)
        print("ðŸ“Š STATISTIQUES TEMPS RÃ‰EL")
        print("="*50)
        print(f"â±ï¸  Uptime: {uptime}")
        print(f"ðŸ”„ VÃ©rifications totales: {self.stats['total_checks']}")
        print(f"ðŸ“ˆ Nouveaux articles: {self.stats['new_articles_found']}")
        print(f"ðŸ¤– Analyses complÃ¨tes: {self.stats['analyses_completed']}")
        print(f"âŒ Erreurs: {self.stats['errors']}")
        print(f"ðŸ’¾ Articles en cache: {len(self.processed_articles)}")
        print("="*50)
    
    def start(self):
        """DÃ©marre la surveillance temps rÃ©el"""
        self.log("ðŸš€ DÃ‰MARRAGE DE LA SURVEILLANCE TEMPS RÃ‰EL", "SUCCESS")
        self.log(f"âš¡ Polling: {self.poll_interval} secondes")
        self.log("â¹ï¸  ArrÃªt: Ctrl+C")
        
        # Initialiser la base de donnÃ©es
        init_db()
        
        # DÃ©marrer le thread de surveillance
        self.running = True
        monitor_thread = threading.Thread(target=self.monitoring_thread, daemon=True)
        monitor_thread.start()
        
        try:
            # Boucle principale avec affichage des stats
            while True:
                time.sleep(30)  # Afficher les stats toutes les 30 secondes
                self.display_stats()
                
        except KeyboardInterrupt:
            self.log("ðŸ›‘ ArrÃªt demandÃ© par l'utilisateur", "WARNING")
        finally:
            self.running = False
            self.log("ðŸ‘‹ Surveillance temps rÃ©el arrÃªtÃ©e", "SUCCESS")
            self.display_stats()


def main():
    """Point d'entrÃ©e principal"""
    # Configuration par dÃ©faut
    poll_interval = 30  # 30 secondes par dÃ©faut
    capital = 25000.0
    
    # Parsing des arguments
    if len(sys.argv) > 1:
        try:
            poll_interval = int(sys.argv[1])
            if poll_interval < 10:
                raise ValueError("L'intervalle doit Ãªtre d'au moins 10 secondes")
        except ValueError as e:
            print(f"âŒ Erreur: {e}")
            print("ðŸ’¡ Usage: python real_time_rss_monitor.py [poll_interval_seconds]")
            print("ðŸ“Š Exemple: python real_time_rss_monitor.py 30")
            sys.exit(1)
    
    print(f"ðŸš€ BERZERK Real-Time RSS Monitor")
    print(f"âš¡ Surveillance quasi-instantanÃ©e avec polling optimisÃ©")
    print(f"ðŸ”„ Intervalle: {poll_interval} secondes")
    print("-" * 70)
    
    # Lancement du monitor
    monitor = RealTimeRSSMonitor(poll_interval, capital)
    monitor.start()


if __name__ == "__main__":
    main() 